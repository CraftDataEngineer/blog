---
title: Data testing - Test EndToEnd sur une pipeline data
date: 2023-01-18
---

## Objectifs :

+ Dans quel context et Pourquoi faut-il les mettre en place ?
  + Si vous apportez une modification d'infrastructure ou de code √† un pipeline, comment savez-vous que vous n'avez rien cass√© ?
  + Les tests de non-r√©gression manuels sont long et certaines fois des erreurs nous √©chappent (l'erreur humaine)
  + 
+ Comment les mettre en place (th√©orie)?
  + Env isol√© ?
  + des donn√©es de test fix√©es et connues en input
  + des donn√©es d'assertions verifi√©es √† la main
  + Rapprochement du m√©tier/PO --> BDD
--> Comment les mettre en place (pratique) ? (futur article)


------------

<!--truncate-->



Les tests des pipelines de donn√©es sont diff√©rents des tests d'autres applications, comme le backend d'un site web.

Il y a une dualit√© de tests dans un contexte data; les tests sur la `donn√©e` ‚ö°Ô∏è sur le `code` :
+ Les tests de qualit√© de donn√©es ont pour r√¥le de : 
  + D√©tecter des anomalies 
  + Signaler des valeurs de donn√©es aberrantes.
+ Les tests sur le code assurent la qualit√© logicielle minimale, ces diff√©rents types de tests sont repr√©sent√©s ci-desous

## Table des mati√®res


* [Rappels : ](#rappels-)
* [Dans quel context ?](#dans-quel-context-)
* [Pourquoi faut-il les mettre en place ?](#pourquoi-faut-il-les-mettre-en-place-)
* [Par ou commencer ?](#par-ou-commencer-)
* [Covention Dalkia](#convention-dalkia)

------------------------------------------------------------------------------------------------------------------------

## Rappels : 

### C'est quoi une pipeline de donn√©e ?

![img.png](static/data-testing/data-pipeline.png)
> üí° Dans le livre [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/) 
> il associe la pipeline de donn√©e au Data Engineering lifecycle, c'est l'endroit ou le Data Engineer intervient le plus
> souvent.

_Les donn√©es entrent en continues d'un c√¥t√© du pipeline, progressent √† travers une s√©rie d'√©tapes et sortent sous la forme 
de rapports, de mod√®les et de vues._   

_Il est utile de conceptualiser le pipeline de donn√©es comme une cha√Æne de fabrication o√π la qualit√©, l'efficacit√©, 
les contraintes et le temps de fonctionnement doivent √™tre g√©r√©s._

> Le pipeline de donn√©es est le c√¥t√© **"op√©rationnel"** de l'analyse des donn√©es.

### Les √©tapes d'une pipeline de donn√©e :

1. **G√©n√©ration / Syst√®me source :** c'est l'origine des donn√©es utilis√©es.  
   _Par exemple_, un syst√®me source peut √™tre : 
   * Un objet IOT
   * Une application qui g√®re une file d'attente de messages
   * Une base de donn√©e transactionnelle  
   
   Souvent en tant que Data Engineer, nous consommons des donn√©es d'un syst√®me source mais nous n'en n'avons pas le contr√¥le.

2. **Stockage :** Vous avez besoin d'un endroit pour stocker :
   *  Les donn√©es du syst√®me source 
   *  Les r√©sultats des transformations
   *  Les donn√©es expos√©es 
   
   Quel type de solution de stockage devez-vous utiliser ? Cela d√©pend de vos cas d'utilisation, des volumes de donn√©es,
   de la fr√©quence d'ingestion, du format et de la taille des donn√©es ing√©r√©es.

3. **Ingestion :** Apr√®s avoir compris la source de donn√©es, les caract√©ristiques du syst√®me source que vous utilisez et 
   la mani√®re dont les donn√©es sont stock√©es, `vous devez rassembler les donn√©es`. L'√©tape suivante du cycle de vie 
   du Data Engineering est l'ingestion des donn√©es √† partir des syst√®mes sources.   
   > üí° Pour cette √©tape il y 2 grands concepts √† connaitre que nous n'addresserons pas dans cet article: 
   * `Batch` VS `Streaming` 
   * `Push` VS `Pull` 
   
4. **Transformation :** Signifie que les donn√©es doivent √™tre chang√©es de leur forme originale en quelque chose d'`utile` pour
   les cas d'utilisation en aval.  
   Sans transformations appropri√©es, les donn√©es resteront inertes, et ne seront pas sous une forme utile pour les 
   rapports, l'analyse ou le ML. Typiquement, l'√©tape de transformation est celle o√π les donn√©es commencent √† cr√©er 
   de la `valeur` pour la consommation des utilisateurs en aval.

5. **Donn√©es de consommation :** Maintenant que les donn√©es ont √©t√© ing√©r√©es, stock√©es et transform√©es en structures 
   coh√©rentes et utiles, il est temps de les `valoriser`.  
   Les valoriser √† travers le utilisations les plus courantes comme :
   * L'`analytique` 
   * Le `ML` 
   * Le `reverse ETL`

Dans l'analyse de donn√©es, il y a deux fa√ßons courantes d'√™tre embarrass√© professionnellement :
*  Laisser des `donn√©es de mauvaise qualit√©` atteindre les utilisateurs.
*  D√©ployer des `changements qui cassent les syst√®mes de production`

Et pour √©viter ces probl√®mes nous allons introduire deux workflows cl√©s.

### Deux Workflows cl√©s : Le Pipeline de Valeur & Le Pipeline d'Innovation

> Ces 2 workflows cl√©s sont d√©finis dans le livre DataOps : https://dkproduction.wpenginepowered.com/wp-content/uploads/2020/11/DK_dataops_book_2nd_edition.pdf

La diff√©rence entre les deux workflows :

_la pipeline de valeur se situe sur l'environnement de `production` tandis que la
pipeline d'innovation est sur une `sandboxe` (Dev)._

Pour le `pipeline de valeur`, lorsque les donn√©es en sortent sous la sous forme d'analyses utiles, de la `valeur est cr√©√©e pour l'organisation`.

Pour le `pipeline d'innovation`, il cherche √† am√©liorer l'analyse des donn√©es en mettant en ≈ìuvre de nouvelles id√©es qui produisent des _"insigths"_ analytiques.

**Pourquoi introduire ces 2 concepts ?** 

Car on pourrait r√©sumer la qualit√© de nos r√©sultats √† : 

`qualit√© de notre service = f(data, code)`

![img_1.png](static/data-testing/data-quality-fonction.png)

Le pipeline de valeur ( pipeline en production ) traite les donn√©es de production qui sont mises √† jours √† differentes fr√©quences, mais le code reste constant ( fix√© / releas√© ).   
‚û°Ô∏è Code constant mais donn√©e variable 

> ‚ö†Ô∏è Le pire sc√©nario dans ce pipeline de valeur est de recevoir des donn√©es de mauvaise qualit√©. 

Le pipeline d'innovation, la donn√©e est control√©e / stable pour √©viter les effets de bords pendant les d√©veloppements.
Par contre le code sera modifi√© r√©guli√®rement durant cette phase pour arriver √† la qualit√© et au fonctionnement 
souhait√©.  
‚û°Ô∏è Code variable mais donn√©e constante (fix√©e). 

> ‚ö†Ô∏è Le pire sc√©nario dans ce pipeline d'innovation est d'introduire des r√©gressions dans les changements du syst√®me 
> (code). 

Il y aura une dualit√© sur les tests : 
+ En production, on retrouvera les tests de donn√©es et monitoring
+ En d√©veloppement, on retrouvera les tests unitaires, fonctionnels, performances ...

![dualit√© de tests](./static/data-testing/dualite-tests.png)

Pour le pipeline d'innovation il est important de mettre en place une strat√©gie de tests, afin de valider le syst√®me et
de nous assurer de ne pas faire de r√©gressions dans de futurs changements.

Les tests que nous pouvons mettre en place : 
 
 
![img.png](static/data-testing/pyramide-test.png)
https://jaayap.github.io/Unity_Best_Practices/Fr/Unit_Test_And_TDD.html

Pour le pipeline de valeur, il faut contr√¥ler la donn√©e en entr√©e, en sortie et √† la fin de chaque √©tapes de traitements.


